{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jifangao/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from measure_disparity import Measure_disparity\n",
    "from mitigate_disparity import relocate_icd9_section\n",
    "from helpers import *\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "db_path = '/media/jifangao/backup/mimic_iii_dataset/mimiciii.sqlite3'\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.90):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return round(m,3), round(m-h,3), round(m+h,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases\n",
    "con = sqlite3.connect(db_path)\n",
    "query = \"\"\"\n",
    "        WITH t AS\n",
    "        (\n",
    "            SELECT *\n",
    "            FROM (\n",
    "                SELECT subject_id, hadm_id, dischtime, \n",
    "                JULIANDAY(MAX(dischtime)) AS last_dischtime, hospital_expire_flag\n",
    "                FROM admissions\n",
    "                GROUP BY subject_id\n",
    "            )\n",
    "            WHERE hospital_expire_flag = 0\n",
    "        )\n",
    "        SELECT t.*, p.dod\n",
    "        FROM t\n",
    "        LEFT JOIN patients p\n",
    "        ON t.subject_id = p.subject_id\n",
    "        WHERE JULIANDAY(p.dod) - last_dischtime > 1\n",
    "        AND JULIANDAY(p.dod) - last_dischtime <= 30\n",
    "        \"\"\"\n",
    "\n",
    "df_cases = pd.read_sql_query(query, con)\n",
    "con.commit()\n",
    "con.close()\n",
    "\n",
    "# cohort\n",
    "con = sqlite3.connect(db_path)\n",
    "query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM (\n",
    "            SELECT subject_id, hadm_id, dischtime, \n",
    "            JULIANDAY(MAX(dischtime)) AS last_dischtime, hospital_expire_flag\n",
    "            FROM admissions\n",
    "            GROUP BY subject_id\n",
    "        )\n",
    "        WHERE hospital_expire_flag = 0\n",
    "        \"\"\"\n",
    "\n",
    "df_cohort = pd.read_sql_query(query, con)\n",
    "con.commit()\n",
    "con.close()\n",
    "\n",
    "# df to save admission infos\n",
    "con = sqlite3.connect(db_path)\n",
    "query = \"\"\"\n",
    "        select ad.hadm_id, pt.subject_id,\n",
    "        ad.ethnicity,\n",
    "        (JulianDay(ad.admittime)-JulianDay(pt.dob))/365 as age,\n",
    "        case when JulianDay(ad.dischtime)-JulianDay(ad.admittime)>30 then 1 else 0 end as long_adm,\n",
    "        0 as re_adm,\n",
    "        hospital_expire_flag as death\n",
    "        from admissions ad, patients pt\n",
    "        where pt.subject_id = ad.subject_id\n",
    "        \"\"\"\n",
    "\n",
    "df_adm = pd.read_sql_query(query, con)\n",
    "con.commit()\n",
    "con.close()\n",
    "\n",
    "# df to save admission infos\n",
    "con = sqlite3.connect(db_path)\n",
    "query = \"\"\"\n",
    "        select ad.hadm_id, pt.subject_id,\n",
    "        ad.ethnicity,\n",
    "        (JulianDay(ad.admittime)-JulianDay(pt.dob))/365 as age,\n",
    "        case when JulianDay(ad.dischtime)-JulianDay(ad.admittime)>30 then 1 else 0 end as long_adm,\n",
    "        0 as re_adm,\n",
    "        hospital_expire_flag as death\n",
    "        from admissions ad, patients pt\n",
    "        where pt.subject_id = ad.subject_id\n",
    "        \"\"\"\n",
    "\n",
    "df_adm = pd.read_sql_query(query, con)\n",
    "con.commit()\n",
    "con.close()\n",
    "\n",
    "# dictionary to save patients and admission dates\n",
    "dic_pat_adm = {}\n",
    "con = sqlite3.connect(db_path)\n",
    "query = \"\"\"\n",
    "        select subject_id, hadm_id, JulianDay(admittime), JulianDay(dischtime)\n",
    "        from admissions\n",
    "        \"\"\"\n",
    "c = con.cursor()\n",
    "c.execute(query)\n",
    "for row in c:\n",
    "    if row[0] in df_cohort.subject_id.values:\n",
    "        if row[0] in dic_pat_adm:\n",
    "            dic_pat_adm[row[0]].append((row[1], row[2], row[3]))\n",
    "        else:\n",
    "            dic_pat_adm[row[0]] = [(row[1], row[2], row[3])]\n",
    "con.commit()\n",
    "con.close()\n",
    "\n",
    "for i in dic_pat_adm:\n",
    "    if len(dic_pat_adm[i]) == 1: continue\n",
    "    dic_pat_adm[i].sort(key=lambda x: x[1])\n",
    "    \n",
    "# dictionary to save admisssions and daignosis\n",
    "dic_admi_dgx = {}\n",
    "con = sqlite3.connect(db_path)\n",
    "query = \"\"\"\n",
    "        select *\n",
    "        from diagnosis_icd\n",
    "        \"\"\"\n",
    "c = con.cursor()\n",
    "c.execute(query)\n",
    "for row in c:\n",
    "    if row[2] in dic_admi_dgx:\n",
    "        dic_admi_dgx[row[2]].append(row[-1])#[:3])\n",
    "    else:\n",
    "        dic_admi_dgx[row[2]] = [row[-1]]\n",
    "con.commit()\n",
    "con.close()\n",
    "\n",
    "# encode race groups\n",
    "dic_race_encode = race_encoder_mimiciii()\n",
    "df_adm[\"ETHNICITY_encoded\"] = df_adm[\"ETHNICITY\"].map(dic_race_encode)\n",
    "df_adm[\"ETHNICITY_encoded\"].value_counts()           \n",
    "dic_pat_race = {}\n",
    "for _, row in df_adm.iterrows():\n",
    "    dic_pat_race[row['SUBJECT_ID']] = row['ETHNICITY_encoded']\n",
    "    \n",
    "# collect dignosis of each patient\n",
    "pat_dgx = {}\n",
    "loc = 150 # number of icd-9 sections\n",
    "for i in dic_pat_adm:\n",
    "    pat_dgx[i] = []\n",
    "    count = 0\n",
    "    for count, j in enumerate(dic_pat_adm[i]):\n",
    "        if count != 0: continue\n",
    "        for k in dic_admi_dgx[j[0]]:\n",
    "            if k == '': continue\n",
    "            dgx = icd_chapter_section(k)\n",
    "            pat_dgx[i].append(dgx)\n",
    "# drop patients that are 90 yr older\n",
    "drop_list = []\n",
    "for i in pat_dgx:\n",
    "    if df_adm[df_adm['HADM_ID']==dic_pat_adm[i][-1][0]].age.values[0] > 90:\n",
    "        drop_list.append(i)\n",
    "        \n",
    "# construct features and outcomes\n",
    "X_total = np.zeros([len(pat_dgx), loc])\n",
    "y_total = []\n",
    "pat_idx = {}\n",
    "n = 0\n",
    "for i in pat_dgx:\n",
    "    pat_idx[i] = n\n",
    "    p = 0\n",
    "    for dx in pat_dgx[i]:\n",
    "        X_total[n, dx] += 1\n",
    "        X_total[n, -1] = dic_pat_race[i]\n",
    "        p += 1\n",
    "    n += 1\n",
    "    if i in df_cases.subject_id.values:\n",
    "        y_total.append(1)\n",
    "    else:\n",
    "        y_total.append(0)\n",
    "y_total = np.array(y_total)\n",
    "\n",
    "# hyperparameters\n",
    "range_concept = 121\n",
    "n_fts_enhanced = 5\n",
    "params = {\n",
    "        'objective': 'binary',\n",
    "        \"max_depth\": 24, \n",
    "        \"learning_rate\" : 0.005, \n",
    "        \"num_leaves\": 128,  \n",
    "        \"n_estimators\": 1000,\n",
    "        'n_jobs':-1,\n",
    "        'subsample':0.90,\n",
    "        'verbose': -1,\n",
    "        'seed': 41\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [1:09:10<00:00, 41.50s/it]\n"
     ]
    }
   ],
   "source": [
    "lst_res_base = []\n",
    "lst_res_new = []\n",
    "dic_race_idx = {'white':0, 'black':1, 'asian':2, 'hispanic':3, 'other':4}\n",
    "race_enhanced = 'black'\n",
    "for seed in tqdm(range(100)):\n",
    "    # split train/testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_total, y_total,\n",
    "                                                        test_size=0.2, random_state=seed)\n",
    "    # baseline model\n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "    lgb_model = lgb.train(params, lgb_train)\n",
    "    y_pred = lgb_model.predict(X_test).reshape(-1, 1)\n",
    "    fair_measure = Measure_disparity(X_train, X_test, y_train, y_test, y_pred, -1, dic_race_idx)\n",
    "    dic_res = fair_measure.evaluate(2, race_enhanced)\n",
    "    lst_res_base.append(dic_res)\n",
    "    \n",
    "    # find important features using Shapley values\n",
    "    explainer = shap.TreeExplainer(model=lgb_model)\n",
    "    X_test_sub = X_test[np.where(X_test[:,-1]==dic_race_idx[race_enhanced])[0], :]\n",
    "    shap_values = explainer.shap_values(X_test_sub)\n",
    "    lst_ft_imp_sort = np.argsort(np.mean(np.abs(shap_values[0]), axis=0))[::-1]\n",
    "    # increase granularity on important features\n",
    "    col_expand = [f for f in lst_ft_imp_sort if f <= range_concept][:n_fts_enhanced]\n",
    "    col_expand = np.sort(col_expand)\n",
    "    dic_col_expand_loc = {}\n",
    "    loc = 150 # number of icd-9 sections\n",
    "    for c in col_expand:\n",
    "        res_reloc = relocate_icd9_section(c+1)\n",
    "        dic_col_expand_loc[c] = (res_reloc[1], loc)\n",
    "        loc += res_reloc[0]\n",
    "    for i in dic_pat_adm:\n",
    "        pat_dgx[i] = []\n",
    "        count = 0\n",
    "        for count, j in enumerate(dic_pat_adm[i]):\n",
    "            if count != 0: continue\n",
    "            for k in dic_admi_dgx[j[0]]:\n",
    "                if k == '': continue\n",
    "                dgx = icd_chapter_section(k)\n",
    "                pat_dgx[i].append(dgx)\n",
    "                if dgx in col_expand:\n",
    "                    pat_dgx[i].append(int(k[:3])-dic_col_expand_loc[dgx][0]+dic_col_expand_loc[dgx][1])\n",
    "    \n",
    "    # construct features and outcomes\n",
    "    X_total_new = np.zeros([len(pat_dgx), loc])\n",
    "    y_total_new = []\n",
    "    pat_idx = {}\n",
    "    n = 0\n",
    "    for i in pat_dgx:\n",
    "        pat_idx[i] = n\n",
    "        p = 0\n",
    "        for dx in pat_dgx[i]:\n",
    "            X_total_new[n, dx] += 1\n",
    "            X_total_new[n, -1] = dic_pat_race[i]\n",
    "            p += 1\n",
    "        n += 1\n",
    "        if i in df_cases.subject_id.values:\n",
    "            y_total_new.append(1)\n",
    "        else:\n",
    "            y_total_new.append(0)\n",
    "    y_total_new = np.array(y_total_new)\n",
    "\n",
    "    # split data - identical split as training/validating the basemodel\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_total_new, y_total_new,\n",
    "                                                        test_size=0.2, random_state=seed)\n",
    "    # model with increased granularity on important features\n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "    lgb_model = lgb.train(params, lgb_train)\n",
    "    y_pred = lgb_model.predict(X_test).reshape(-1, 1)\n",
    "    fair_measure = Measure_disparity(X_train, X_test, y_train, y_test, y_pred, -1, dic_race_idx)\n",
    "    dic_res = fair_measure.evaluate(2, race_enhanced)\n",
    "    lst_res_new.append(dic_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUPR, AUROC before mitigation\n",
      "black: 0.137 (0.13, 0.143), 0.81 (0.804, 0.816)\n",
      "Other: 0.17 (0.167, 0.172), 0.821 (0.82, 0.822)\n",
      "\n",
      "AUPR, AUROC after mitigation\n",
      "black: 0.144 (0.136, 0.151), 0.816 (0.81, 0.821)\n",
      "Other: 0.171 (0.168, 0.173), 0.828 (0.827, 0.83)\n"
     ]
    }
   ],
   "source": [
    "dic_race_lst_res = {k:[[],[],[],[]] for k in [race_enhanced, 'Other', 'Disparity']}\n",
    "for res in lst_res_base:\n",
    "    for race in res:\n",
    "        for i,v in enumerate(res[race]):\n",
    "            dic_race_lst_res[race][i].append(v)\n",
    "print(\"AUPR, AUROC before mitigation\")\n",
    "for race in [race_enhanced, 'Other']:\n",
    "    auroc_mean, auroc_lower, auroc_upper = mean_confidence_interval(dic_race_lst_res[race][0])\n",
    "    aupr_mean, aupr_lower, aupr_upper = mean_confidence_interval(dic_race_lst_res[race][1])\n",
    "    print(f\"{race}: {aupr_mean} ({aupr_lower}, {aupr_upper}), {auroc_mean} ({auroc_lower}, {auroc_upper})\")\n",
    "    \n",
    "dic_race_lst_res = {k:[[],[],[],[]] for k in [race_enhanced, 'Other', 'Disparity']}\n",
    "for res in lst_res_new:\n",
    "    for race in res:\n",
    "        for i,v in enumerate(res[race]):\n",
    "            dic_race_lst_res[race][i].append(v)\n",
    "print(\"\\nAUPR, AUROC after mitigation\")\n",
    "for race in [race_enhanced, 'Other']:\n",
    "    auroc_mean, auroc_lower, auroc_upper = mean_confidence_interval(dic_race_lst_res[race][0])\n",
    "    aupr_mean, aupr_lower, aupr_upper = mean_confidence_interval(dic_race_lst_res[race][1])\n",
    "    print(f\"{race}: {aupr_mean} ({aupr_lower}, {aupr_upper}), {auroc_mean} ({auroc_lower}, {auroc_upper})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
